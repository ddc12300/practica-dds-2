---
title: "Actividad Evaluable 2: Data Driven Security – CyberSecurity Management"
author: "Tu Nombre y el de tu compañero/a"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: true
  pdf_document: default
---

# Introducción

Breve descripción de la actividad y los objetivos a alcanzar.

# Pregunta 1: Datos Elegantes + Análisis de Datos con Web Scrapping

## 1. Descargar y almacenar la página web

```{r}
# Cargar las librerías necesarias
library(httr)
library(XML)

# Definir la URL de la página a descargar
url <- "https://www.mediawiki.org/wiki/MediaWiki"

# Utilizar la función GET() para descargar el contenido de la página
pagina_web <- GET(url)

# Verificar que la descarga fue exitosa
if (status_code(pagina_web) == 200) {
  # Convertir el contenido de la página a un formato que pueda ser tratado en R
  contenido_html <- content(pagina_web, as = "text")
  # Parsear el contenido HTML a XML para facilitar su análisis
  contenido_xml <- htmlParse(contenido_html, asText = TRUE)
  # Guardar el contenido XML en un archivo .xml
  saveXML(contenido_xml, file = "pagina_web.xml")
} else {
  stop("La página web no pudo ser descargada correctamente.")
}
```

## 2. Analizar el título de la página

```{r}
# Asegúrate de que las librerías necesarias estén cargadas
library(XML)

# Utilizar XPath para encontrar el título de la página
titulo <- xpathSApply(contenido_xml, "//title", xmlValue)

# Imprimir el título para verificar que se ha extraído correctamente
print(titulo)
```

## 3. Buscar todos los enlaces

```{r}
# Asegúrate de que las librerías necesarias estén cargadas
library(XML)

# Leer el contenido XML desde el archivo si no está ya cargado en la sesión
# contenido_xml <- readRDS(file = "pagina_web.rds")

# Utilizar xpathSApply para extraer todos los atributos 'href' de los enlaces
links_href <- xpathSApply(contenido_xml, "//a", xmlGetAttr, "href")

# Utilizar xpathSApply para extraer todo el texto de los enlaces
links_text <- xpathSApply(contenido_xml, "//a", xmlValue)

# Función para reemplazar NULL o cadenas vacías con "NA"
replace_with_na <- function(x) {
  sapply(x, function(item) {
    if (is.null(item) || trimws(item) == "") {
      return(NA)
    } else {
      return(item)
    }
  })
}

# Reemplazar valores NULL o cadenas vacías en href y texto
links_href <- replace_with_na(links_href)
links_text <- replace_with_na(links_text)

# Crear un data frame con los textos y URLs de los enlaces
df_enlaces <- data.frame(Texto = links_text, URL = links_href, stringsAsFactors = FALSE)

# Imprimir los primeros enlaces para verificar
View(df_enlaces)
```

## 4. Generar tabla de enlaces

```{r}

# Limpiar los espacios en blanco al principio y al final del texto de los enlaces
df_enlaces$Texto <- sapply(df_enlaces$Texto, trimws)

# Rellenamos valores na con NA texto
df_enlaces$Texto[is.na(df_enlaces$Texto)] <- 'NA'
df_enlaces$URL[is.na(df_enlaces$URL)] <- 'NA'

# Agrupar por Texto y URL y contar las ocurrencias
df_enlaces_con_frecuencia <- aggregate(cbind(Frecuencia = df_enlaces$URL) ~ Texto + URL, data = df_enlaces, FUN = length)

# Ordenar el data frame por Frecuencia de mayor a menor para mejor visualización
df_enlaces_con_frecuencia <- df_enlaces_con_frecuencia[order(-df_enlaces_con_frecuencia$Frecuencia), ]

# Imprimir la tabla para verificar
View(df_enlaces_con_frecuencia)
```

## 5. Verificar si los enlaces están activos

```{r}
# Asegúrate de que la librería httr esté cargada
library(httr)

# Asumiendo que ya tienes el data frame df_enlaces con las columnas Texto y URL
# Definir el dominio base para construir las URLs absolutas
base_url <- "https://www.mediawiki.org"

# Crear un nuevo data frame para almacenar los resultados, si no existe
if (!exists("df_enlaces_estado")) {
  df_enlaces_estado <- df_enlaces
  df_enlaces_estado$status_code <- NA  # Inicializar la columna de códigos de estado
}

# Iterar sobre cada enlace y verificar el estado si no se ha verificado previamente
for (i in seq_len(nrow(df_enlaces_estado))) {
  # Si ya se verificó el estado de este enlace, saltar a la siguiente iteración
  if (!is.na(df_enlaces_estado$status_code[i])) {
    next
  }
  
  # Extraer la URL actual
  url <- df_enlaces_estado$URL[i]
  
  # Verificar si la URL es relativa y convertirla a absoluta si es necesario
  if (startsWith(url, "//")) {
    url <- paste0("https:", url)
  } else if (startsWith(url, "/")) {
    url <- paste0(base_url, url)
  } else if (startsWith(url, "#")) {
    url <- base_url # URL interna con ancla
  }
  
  # Realizar una petición HEAD para obtener el código de estado
  response <- tryCatch({
    HEAD(url)
  }, error = function(e) {
    list(status_code = NA) # En caso de error, asignar NA
  })
  
  # Almacenar el código de estado en el nuevo data frame
  df_enlaces_estado$status_code[i] <- response$status_code
  
  # Imprimir el índice del enlace y el código de estado para seguimiento
  cat("Enlace", i, "de", nrow(df_enlaces_estado), "-", url, "Estado:", df_enlaces_estado$status_code[i], "\n")
  
  # Pausar brevemente para evitar ser bloqueado por el servidor
  Sys.sleep(3)
}

# Imprimir los resultados para verificar
View(df_enlaces_estado)
```

# Pregunta 2: Infografía con gráficos

## Histograma de frecuencia de enlaces

```{r}
# Asumiendo que df_enlaces_estado contiene la columna URL y status_code
library(ggplot2)

# Agregar una nueva columna que clasifique cada URL como absoluta o relativa
df_enlaces_estado$TipoURL <- ifelse(grepl("^http", df_enlaces_estado$URL), "Absoluta", "Relativa")

# Crear un gráfico de barras con ggplot2
grafico_barras_urls <- ggplot(df_enlaces_estado, aes(x = TipoURL, fill = TipoURL)) +
  geom_bar() + # Usar geom_bar para gráfico de barras
  geom_text(aes(label=after_stat(count)), stat='count', vjust=-0.5) + # Añadir el valor encima de las barras
  labs(title = "Frecuencia de URLs Absolutas y Relativas",
       x = "Tipo de URL",
       y = "Frecuencia") +
  scale_fill_manual(values = c("Absoluta" = "blue", "Relativa" = "red"))

# Mostrar el gráfico de barras
print(grafico_barras_urls)
```

## Gráfico de barras de enlaces a otros dominios

```{r}
# Asumiendo que df_enlaces_estado contiene la columna URL
# Definir el dominio base para la comparación
dominio_base <- "https://www.mediawiki.org"

# Función para determinar si la URL apunta al dominio base o a otro dominio
es_dominio_base <- function(url) {
  # Comprobar si la URL es relativa
  if (startsWith(url, "/")) {
    return(TRUE)
  }
  # Comprobar si la URL es absoluta y pertenece al dominio base
  dominio <- sub("^(https?://[^/]+).*", "\\1", url)
  return(dominio == dominio_base)
}

# Aplicar la función a cada URL para clasificar los enlaces
df_enlaces_estado$Destino <- sapply(df_enlaces_estado$URL, es_dominio_base)

# Convertir los resultados booleanos a factores con etiquetas "Mediawiki" y "Otro Dominio"
df_enlaces_estado$Destino <- ifelse(df_enlaces_estado$Destino, "Mediawiki", "Otro Dominio")

# Contar la cantidad de enlaces que apuntan a cada destino
conteo_destinos <- table(df_enlaces_estado$Destino)

# Crear un data frame a partir del conteo para el gráfico
df_conteo_destinos <- as.data.frame(conteo_destinos)

# Crear un gráfico de barras con ggplot2
grafico_barras_destinos <- ggplot(df_conteo_destinos, aes(x = Var1, y = Freq, fill = Var1)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Freq), vjust = -0.5) +
  labs(title = "Suma de Enlaces por Destino",
       x = "Destino",
       y = "Cantidad de Enlaces") +
  scale_fill_manual(values = c("Mediawiki" = "green", "Otro Dominio" = "orange"))

# Mostrar el gráfico de barras
print(grafico_barras_destinos)
```

## Gráfico de tarta de Status HTTP

```{r}
# Asegúrate de que la librería ggplot2 esté cargada
library(ggplot2)

# Contar la frecuencia de cada código de estado
conteo_status <- table(df_enlaces_estado$status_code)

# Crear un data frame a partir del conteo para el gráfico
df_conteo_status <- as.data.frame(conteo_status)

# Renombrar las columnas para que sean más descriptivas
names(df_conteo_status) <- c("status_code", "count")

# Crear un gráfico de tarta con ggplot2
grafico_tarta_status <- ggplot(df_conteo_status, aes(x = "", y = count, fill = factor(status_code))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar(theta = "y") +
  theme_void() +
  theme(legend.title = element_blank()) +
  labs(title = "Distribución de Códigos de Estado HTTP", # Añadir título aquí
       fill = "Status Code") +
  geom_text(aes(label = paste0(round(count / sum(count) * 100, 1), "%")), 
            position = position_stack(vjust = 0.5))

# Mostrar el gráfico de tarta
print(grafico_tarta_status)
```

```{r}
# Cargar la librería necesaria para organizar los gráficos
library(gridExtra)

# Combinar los gráficos en una sola figura
grid.arrange(grafico_barras_urls, grafico_barras_destinos, grafico_tarta_status, ncol = 2)
```

# Pregunta 3: Análisis de logs de servidor usando R (parte II)

## Obtención y carga de los Datos


```{r dataset}
library(readr)
data <- read_table("epa-http.csv", col_names = FALSE)
colnames(data) <- c("IP", "Timestamp", "Tipo", "URL", "Protocolo", "Respuesta", "Bytes")
View(data)
```

## Limpieza de los Datos

```{r}
# Código para limpiar y preparar los datos

data$IP <- trimws(data$IP)
data$Timestamp <- sub("^\\[", '', data$Timestamp)
data$Timestamp <- sub("\\]$", '', data$Timestamp)
data$Timestamp <- as.POSIXct(data$Timestamp, format = "%d:%H:%M:%S")
data$Tipo <- sub('"', '', data$Tipo)
data$URL <- trimws(data$URL)
data$Protocolo <- sub('"', '', data$Protocolo)
data$Respuesta <- as.numeric(data$Respuesta)
data$Bytes <- as.numeric(data$Bytes)

View(data)
```

## Exploración de Datos

```{r}
# Código para explorar los datos y obtener estadísticas
usuarios_unicos <- unique(data$IP)
print(paste("Hay", length(usuarios_unicos), "usuarios unicos"))

usuarios_con_error <- unique(data$IP[data$Respuesta >= 400 & data$Respuesta <= 599])
print(paste("Número de usuarios únicos con error:", length(usuarios_con_error)))

usuarios_sin_error <- setdiff(usuarios_unicos, usuarios_con_error)
print(paste("Número de usuarios únicos sin error:", length(usuarios_sin_error)))

codigos_error_unicos <- unique(data$Respuesta[data$Respuesta >= 400 & data$Respuesta <= 599])
print(paste("Código de error:", codigos_error_unicos) )
```

## Análisis de Datos

```{r}
library(stringr)
# Código para analizar los tipos de peticiones HTTP
data_freq_tipo <- as.data.frame(table(data$Tipo))
colnames(data_freq_tipo) <- c("Tipo" , "Frecuencia")
View(data_freq_tipo)


# Filtrar datos donde el valor de data$URL termina en ".jpg o .gif o .xbm"
datos_gif <- subset(data, str_detect(data$URL, "\\.gif"))
datos_jpg <- subset(data, str_detect(data$URL, "\\.jpg"))
datos_xbm <- subset(data, str_detect(data$URL, "\\.xbm"))

datos_combinados <- rbind(datos_gif, datos_jpg, datos_xbm)


# Crear un data frame con la frecuencia de respuestas para los datos filtrados
data_freq_tipo_2 <- as.data.frame(table(datos_combinados$Tipo))
colnames(data_freq_tipo_2) <- c("Tipo", "Frecuencia")

# Ver el data frame resultante
View(data_freq_tipo_2)
```

## Visualización de Resultados

```{r}
# Asegúrate de que la librería ggplot2 esté cargada
library(ggplot2)


# Crear un gráfico de tarta con ggplot2
grafico_tarta_tipo <- ggplot(data_freq_tipo, aes(x = "", y = Frecuencia, fill = factor(Tipo))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar(theta = "y") +
  theme_void() +
  theme(legend.title = element_blank()) +
  labs(title = "Distribución de Códigos de Tipo de peticion HTTP", # Añadir título aquí
       fill = "Status Code") +
  geom_text(aes(label = paste0(round(Frecuencia / sum(Frecuencia) * 100, 1), "%")), 
            position = position_stack(vjust = 0.5))

# Mostrar el gráfico de tarta
print(grafico_tarta_tipo)
```

```{r}
# Asegúrate de que la librería ggplot2 esté cargada
library(ggplot2)

peticiones_total <- sum(data_freq_tipo$Frecuencia)

peticiones_imagen <- sum(data_freq_tipo_2$Frecuencia)

otras_peticiones <- peticiones_total - peticiones_imagen


# Datos de ejemplo
valores <- c(peticiones_imagen, otras_peticiones)
categorias <- c("Peticiones imagen", "Otras peticiones")

# Calcular porcentajes
porcentajes <- round(valores / sum(valores) * 100, 1)

# Crear el gráfico circular
pie(valores, labels = paste(categorias, "\n", porcentajes, "%"), col = c("red", "blue"), main = "Gráfico Circular")

# Agregar leyenda
legend("topright", legend = categorias, fill = c("red", "blue"))

```

```{r}
# Asegúrate de que las librerías necesarias estén cargadas
library(ggplot2)
library(dplyr)
library(lubridate)

# Ajustar la conversión de la columna Timestamp al formato correcto
# Asumiendo que el formato de la fecha es "YYYY-MM-DD HH:MM:SS"
data$Timestamp <- ymd_hms(data$Timestamp)

# Agrupar los datos por intervalos de tiempo (por ejemplo, por hora)
# y contar el número de peticiones en cada intervalo
data_agrupada <- data %>%
  mutate(Hour = floor_date(Timestamp, "hour")) %>%
  group_by(Hour) %>%
  summarise(NumeroPeticiones = n())

# Crear un gráfico de líneas para visualizar el número de peticiones a lo largo del tiempo
ggplot(data_agrupada, aes(x = Hour, y = NumeroPeticiones)) +
  geom_line() +
  labs(title = "Número de peticiones servidas a lo largo del tiempo",
       x = "Hora",
       y = "Número de Peticiones") +
  theme_minimal()
```

## Clústering de datos

```{r}

```


## Representación visual de clústering

```{r}
# Código para representar visualmente los clústeres
```
